{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv11 Medical Object Detection (Optimized)\n",
    "\n",
    "This notebook is an optimized version for training YOLOv11 on the Thesis Dataset.\n",
    "**Improvements:**\n",
    "- Local & Colab compatibility (no hardcoded `/content/` paths).\n",
    "- Auto-detects GPU/CPU.\n",
    "- Structured configuration and improved readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import yaml\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPyImage\n",
    "\n",
    "# Check for local vs Colab environment and install dependencies if needed\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab environment.\")\n",
    "    # Uncomment the line below to install dependencies automatically in Colab/Fresh env\n",
    "    # !pip install ultralytics roboflow thop \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running in Local environment.\")\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Device Configuration\n",
    "DEVICE = '0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "if DEVICE == '0':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Set your hyperparameters and API keys here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "ROBOFLOW_API_KEY = \"e3DAOLcZTZkR7VlO5jY2\"  # Make sure to keep this secure!\n",
    "PROJECT_NAME = \"mkthesis-v4-kfvrb\"\n",
    "VERSION_NUMBER = 3\n",
    "\n",
    "MODEL_NAME = \"yolo11n.pt\" # Pretrained model to start from\n",
    "IMG_SIZE = 640\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 300\n",
    "PATIENCE = 10\n",
    "OPTIMIZER = 'SGD'\n",
    "LR0 = 0.01\n",
    "LRF = 0.1\n",
    "\n",
    "# Output Directory\n",
    "RUN_NAME = 'train_optimized'\n",
    "PROJECT_DIR = Path('runs/detect')\n",
    "SAVE_DIR = PROJECT_DIR / RUN_NAME\n",
    "print(f\"Training results will be saved to: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Roboflow\n",
    "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "project = rf.workspace(\"pcithesis\").project(PROJECT_NAME)\n",
    "version = project.version(VERSION_NUMBER)\n",
    "\n",
    "# Download Dataset\n",
    "dataset = version.download(\"yolov11\")\n",
    "\n",
    "# dataset.location contains the absolute path to the downloaded dataset\n",
    "DATASET_DIR = Path(dataset.location)\n",
    "DATA_YAML = DATASET_DIR / \"data.yaml\"\n",
    "\n",
    "print(f\"Dataset downloaded to: {DATASET_DIR}\")\n",
    "print(f\"Data YAML: {DATA_YAML}\")\n",
    "\n",
    "# Verify data.yaml content\n",
    "with open(DATA_YAML, 'r') as f:\n",
    "    yaml_content = yaml.safe_load(f)\n",
    "    print(\"\\nYAML Content:\")\n",
    "    print(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = YOLO(MODEL_NAME)\n",
    "\n",
    "# Train Model\n",
    "results = model.train(\n",
    "    data=str(DATA_YAML),\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMG_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    optimizer=OPTIMIZER,\n",
    "    lr0=LR0,\n",
    "    lrf=LRF,\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005,\n",
    "    patience=PATIENCE,\n",
    "    device=DEVICE,\n",
    "    project=str(PROJECT_DIR),\n",
    "    name=RUN_NAME,\n",
    "    save=True,\n",
    "    plots=True,\n",
    "    verbose=True,\n",
    "    exist_ok=True # Overwrite if exists, or set False to increment run name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model\n",
    "best_weight_path = SAVE_DIR / 'weights' / 'best.pt'\n",
    "print(f\"Loading best weights from: {best_weight_path}\")\n",
    "\n",
    "if best_weight_path.exists():\n",
    "    best_model = YOLO(best_weight_path)\n",
    "else:\n",
    "    print(\"Warning: Best weights not found. Using current model state.\")\n",
    "    best_model = model\n",
    "\n",
    "# Model Info\n",
    "best_model.info(verbose=True)\n",
    "\n",
    "# Calculate GFLOPs & Params (using thop if installed, or torch)\n",
    "try:\n",
    "    from thop import profile\n",
    "    dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE if DEVICE != 'cpu' else 'cpu')\n",
    "    best_model.model.to(DEVICE if DEVICE != 'cpu' else 'cpu')\n",
    "    macs, params = profile(best_model.model, inputs=(dummy_input, ), verbose=False)\n",
    "    gflops = macs / 1e9\n",
    "    print(f\"GFLOPs: {gflops:.2f}\")\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "except ImportError:\n",
    "    print(\"thop library not installed. Skipping GFLOPs calculation.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating GFLOPs: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate on Test Set\n",
    "test_metrics = best_model.val(\n",
    "    data=str(DATA_YAML),\n",
    "    split='test',\n",
    "    imgsz=IMG_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    device=DEVICE,\n",
    "    project=str(PROJECT_DIR),\n",
    "    name=f\"{RUN_NAME}_val\",\n",
    "    plots=True\n",
    ")\n",
    "\n",
    "# Extract Key Metrics\n",
    "precision = test_metrics.box.mp\n",
    "recall = test_metrics.box.mr\n",
    "map50 = test_metrics.box.map50\n",
    "map50_95 = test_metrics.box.map\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "print(\"\\n----- Test Metrics -----\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1_score:.4f}\")\n",
    "print(f\"mAP50:     {map50:.4f}\")\n",
    "print(f\"mAP50-95:  {map50_95:.4f}\")\n",
    "\n",
    "# Save Metrics to Text File\n",
    "with open(\"test_metrics.txt\", \"w\") as f:\n",
    "    f.write(\"----- Test Metrics -----\\n\")\n",
    "    f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "    f.write(f\"Recall:    {recall:.4f}\\n\")\n",
    "    f.write(f\"F1 Score:  {f1_score:.4f}\\n\")\n",
    "    f.write(f\"mAP50:     {map50:.4f}\\n\")\n",
    "    f.write(f\"mAP50-95:  {map50_95:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Per-Class Metrics to CSV\n",
    "try:\n",
    "    num_classes = len(test_metrics.box.p)\n",
    "    class_names = [best_model.names[i] for i in range(num_classes)]\n",
    "    \n",
    "    df_metrics = pd.DataFrame({\n",
    "        \"Class ID\": range(num_classes),\n",
    "        \"Class Name\": class_names,\n",
    "        \"Precision\": test_metrics.box.p,\n",
    "        \"Recall\": test_metrics.box.r,\n",
    "        \"F1 Score\": test_metrics.box.f1,\n",
    "        \"mAP50\": test_metrics.box.ap50,\n",
    "        \"mAP50-95\": test_metrics.box.ap\n",
    "    })\n",
    "    \n",
    "    csv_path = \"per_class_metrics.csv\"\n",
    "    df_metrics.to_csv(csv_path, index=False)\n",
    "    print(f\"✅ Per-class metrics exported to {csv_path}\")\n",
    "    display(df_metrics)\n",
    "except Exception as e:\n",
    "    print(f\"Error exporting per-class metrics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Confusion Matrix\n",
    "try:\n",
    "    # Confusion matrix is automatically saved by val() method in the run folder\n",
    "    # Access raw matrix if needed\n",
    "    cm = test_metrics.confusion_matrix.matrix\n",
    "    np.savetxt(\"confusion_matrix.csv\", cm, delimiter=\",\", fmt=\"%.0f\")\n",
    "    print(\"✅ Confusion matrix exported to confusion_matrix.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not export custom confusion matrix CSV ({e})\")\n",
    "\n",
    "# Display Training Results (Loss, mAP curves)\n",
    "results_csv = SAVE_DIR / 'results.csv'\n",
    "if results_csv.exists():\n",
    "    df_results = pd.read_csv(results_csv)\n",
    "    df_results.columns = [c.strip() for c in df_results.columns]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(df_results['epoch'], df_results['train/box_loss'], label='Train Box Loss')\n",
    "    plt.plot(df_results['epoch'], df_results['val/box_loss'], label='Val Box Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Box Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    \n",
    "    # mAP Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(df_results['epoch'], df_results['metrics/mAP50(B)'], label='mAP50')\n",
    "    plt.plot(df_results['epoch'], df_results['metrics/mAP50-95(B)'], label='mAP50-95')\n",
    "    plt.legend()\n",
    "    plt.title('mAP Metrics')\n",
    "    plt.xlabel('Epoch')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Results CSV not found yet (training might not have started or failed).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results to Zip\n",
    "# Construct Zip Filename with Datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "zip_filename = f\"{MODEL_NAME}_{PROJECT_NAME}_v{VERSION_NUMBER}_{timestamp}\"\n",
    "zip_path = Path(f\"{zip_filename}.zip\")\n",
    "\n",
    "print(f\"Zipping results to {zip_path}...\")\n",
    "\n",
    "# Create Zip Archive of the Save Directory\n",
    "try:\n",
    "    shutil.make_archive(zip_filename, 'zip', SAVE_DIR)\n",
    "    print(f\"✅ Zip created successfully: {zip_path.resolve()}\")\n",
    "\n",
    "    # Trigger Download if in Colab\n",
    "    if IN_COLAB:\n",
    "        from google.colab import files\n",
    "        files.download(str(zip_path))\n",
    "        print(\"Download triggered.\")\n",
    "    else:\n",
    "        print(f\"File saved locally at: {zip_path.absolute()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating or downloading zip: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "Training is complete. Metrics and models have been exported."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
